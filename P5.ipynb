{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haleh/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import sys\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "import pylab as pl\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pprint.pprint (data_dict.keys())\n",
    "# print (\"There are \", len(data_dict.keys()), \" executives in Enron Dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Select what features you'll use.\n",
    "features_list is a list of strings, each of which is a feature name.<br>\n",
    "The first feature must be \"poi\".<br>\n",
    "You will need to use more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "\n",
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'salary_to_avg',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'loan_advances',\n",
    "                 'bonus',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'deferred_income',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'exercised_stock_options',\n",
    "                 'other',\n",
    "                 'long_term_incentive',\n",
    "                 'restricted_stock',\n",
    "                 'director_fees',\n",
    "                 'to_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'shared_receipt_with_poi']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Remove outliers\n",
    "based on mini-project in outlier lessons, i know there is a key \"Total\", which holds the total value for all other values. <br>\n",
    "Another one is the key \"THE TRAVEL AGENCY IN THE PARK\".<br>\n",
    "I'm going to exclude both of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pprint.pprint (data_dict[\"TOTAL\"], width=1)\n",
    "# pprint.pprint (data_dict[\"THE TRAVEL AGENCY IN THE PARK\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop(\"TOTAL\")\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Create new feature(s)\n",
    "Store to my_dataset for easy export below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284087.542553\n"
     ]
    }
   ],
   "source": [
    "total = 0.0\n",
    "avg=0.0\n",
    "for k,v in data_dict.iteritems():\n",
    "    if v['salary'] == \"NaN\" or v['salary']<0:\n",
    "        v['salary_to_avg'] = \"NaN\"\n",
    "    else:\n",
    "        total += v['salary']\n",
    "avg= total/94        \n",
    "print avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for key, value in data_dict.iteritems():\n",
    "#     if value['bonus'] == \"NaN\" or value['salary'] == \"Nan\":\n",
    "#         value['bonus_salary_R'] = \"NaN\"\n",
    "#     else:\n",
    "#         value['bonus_salary_R'] = float(value['bonus']) / float(value['salary'])\n",
    "\n",
    "# for key, value in data_dict.iteritems():\n",
    "#     if value[\"from_this_person_to_poi\"] == \"NaN\" or value[\"to_messages\"] == \"Nan\":\n",
    "#         value['from_this_person_to_poi_percent'] = \"NaN\"\n",
    "#     else:\n",
    "#         value['from_this_person_to_poi_percent'] = float(value['from_this_person_to_poi'])*100 / float(value['to_messages'])\n",
    "\n",
    "# for key, value in data_dict.iteritems():\n",
    "#     if value[\"from_poi_to_this_person\"] == \"NaN\" or value[\"to_messages\"] == \"Nan\":\n",
    "#         value['from_poi_to_this_person_Percent'] = \"NaN\"\n",
    "#     else:\n",
    "#         value['from_poi_to_this_person_Percent'] = float(value['from_poi_to_this_person'])*100 / float(value['to_messages'])\n",
    "\n",
    "for key, value in data_dict.iteritems():\n",
    "    if value[\"salary\"] == \"NaN\" :\n",
    "        value['salary_to_avg'] = \"NaN\"\n",
    "    else:\n",
    "        value['salary_to_avg'] = float(value['salary']) / float(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k,v in data_dict.iteritems():\n",
    "#     print v['salary_to_avg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dataset = data_dict\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.DataFrame.from_dict(my_dataset, orient = 'index')\n",
    "# df = df[features_list]\n",
    "# df = df.replace('NaN', np.nan)\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Try a varity of classifiers\n",
    "Please name your classifier clf for easy export below.\n",
    "Note that if you want to do PCA or other multi-stage operations,\n",
    "you'll need to use Pipelines. For more info:\n",
    "http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels,  \n",
    "                                                                            test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#minmax scaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaled_feature_train = scaler.fit_transform(features_train)\n",
    "# scaled_feature_test = scaler.fit_transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# def GridSearch (classifier, score_type, name):\n",
    "#     pipe = Pipeline([\n",
    "#     ('reduce_dim', PCA()),\n",
    "#     ('classify', classifier)\n",
    "#     ])\n",
    "#     N_FEATURES_OPTIONS = [5,6,7,8]\n",
    "#     param_grid = [\n",
    "#         {\n",
    "#             'reduce_dim': [PCA(iterated_power=7, random_state=42)],\n",
    "#             'reduce_dim__n_components': N_FEATURES_OPTIONS\n",
    "#         },\n",
    "#         {\n",
    "#             'reduce_dim': [SelectKBest()],\n",
    "#             'reduce_dim__k': N_FEATURES_OPTIONS\n",
    "#         },\n",
    "#     ]\n",
    "#     reducer_labels = ['PCA',  'KBest(chi2)']\n",
    "#     grid = GridSearchCV(pipe, cv=5, param_grid=param_grid,  scoring = score_type)\n",
    "#     grid.fit(scaled_feature_train, labels_train)\n",
    "#     clf_grid = grid.best_estimator_\n",
    "#     print (grid.best_params_)\n",
    "#     mean_scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "    \n",
    "#     mean_scores = mean_scores.reshape(1, -1, len(N_FEATURES_OPTIONS))\n",
    "#     mean_scores = mean_scores.max(axis=0)\n",
    "#     print (mean_scores)\n",
    "\n",
    "#     bar_offsets = (np.arange(len(N_FEATURES_OPTIONS)) *\n",
    "#                    (len(reducer_labels) + 1) + .5)\n",
    "\n",
    "#     plt.figure()\n",
    "#     COLORS = ['g', 'r']\n",
    "#     for i, (label, reducer_scores) in enumerate(zip(reducer_labels, mean_scores)):\n",
    "#         plt.bar(bar_offsets + i, reducer_scores, label=label, color=COLORS[i])\n",
    "\n",
    "#     plt.title(\"PCA vs. KBest\")\n",
    "#     plt.xlabel('Number of features')\n",
    "#     plt.xticks(bar_offsets + len(reducer_labels)/3, N_FEATURES_OPTIONS)\n",
    "#     plt.ylabel(name)\n",
    "#     plt.ylim((0, 1))\n",
    "#     plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "#     return plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # #Naive Bayes\n",
    "# classifier =GaussianNB()\n",
    "\n",
    "# # GridSearch (classifier, 'accuracy', 'Accuracy')\n",
    "# # GridSearch (classifier, 'precision', 'Precision')\n",
    "# GridSearch (classifier, 'recall', 'Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# classifier = LinearSVC()\n",
    "\n",
    "# # print (GridSearch (classifier,'recall',  'Accuracy'))\n",
    "# print (GridSearch (classifier,'recall',  'Recall'))\n",
    "# print (GridSearch (classifier,'precision', 'Precision'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classifier = DecisionTreeClassifier()\n",
    "\n",
    "# # print (GridSearch (classifier,'recall',  'Accuracy'))\n",
    "# print (GridSearch (classifier,'recall',  'Recall'))\n",
    "# print (GridSearch (classifier,'precision', 'Precision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-5efed52d7254>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGridSearch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGridSearch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m'Recall'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGridSearch\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearch' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = KNeighborsClassifier()\n",
    "\n",
    "print (GridSearch (classifier,'recall',  'Accuracy'))\n",
    "print (GridSearch (classifier,'recall',  'Recall'))\n",
    "print (GridSearch (classifier,'precision', 'Precision'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'salary_to_avg',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'loan_advances',\n",
    "                 'bonus',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'exercised_stock_options',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_this_person_to_poi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. \n",
    "\n",
    "#Creat the pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=100, random_state = 42)\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', SelectKBest()),\n",
    "#     ('scaling', MinMaxScaler()),\n",
    "    ('classify', DecisionTreeClassifier(splitter='best', random_state=42))\n",
    "])\n",
    "param_grid_DT = [\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': [3,6,9],\n",
    "        'classify__criterion':['gini', 'entropy'],\n",
    "        'classify__class_weight':[None, 'balanced'],\n",
    "        'classify__min_samples_split' : [2,3,4,5],\n",
    "        'classify__max_leaf_nodes': [None, 5],\n",
    "        'classify__min_samples_leaf': [4, 2]\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "#gridsearch\n",
    "grid = GridSearchCV(pipe, cv=sss, param_grid=param_grid_DT, scoring='f1')\n",
    "gs = grid.fit(features_train, labels_train)\n",
    "clf =gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. \n",
    "\n",
    "#Creat the pipeline\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=100, random_state = 42)\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', SelectKBest()),\n",
    "#     ('scaling', MinMaxScaler()),\n",
    "    ('classify', KNeighborsClassifier())\n",
    "])\n",
    "param_grid_DT = [\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest()],\n",
    "        'reduce_dim__k': [3, 6],\n",
    "        'classify__algorithm' : ['auto'], \n",
    "        'classify__leaf_size' : [30, 50],\n",
    "        'classify__n_neighbors' : [5, 10], \n",
    "        'classify__p' : [1, 2], \n",
    "        'classify__weights' : ['uniform', 'distance'],\n",
    "        'classify__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "]\n",
    "\n",
    "#gridsearch\n",
    "grid = GridSearchCV(pipe, cv=sss, param_grid=param_grid_DT, scoring='recall')\n",
    "gs = grid.fit(features_train, labels_train)\n",
    "clf =gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "test_classifier(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "# ### using our testing script. \n",
    "\n",
    "# #Creat the pipeline\n",
    "# from sklearn.model_selection import StratifiedShuffleSplit\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# sss = StratifiedShuffleSplit(n_splits=100, random_state = 42)\n",
    "# pipe = Pipeline([\n",
    "#     ('reduce_dim', SelectKBest()),\n",
    "# #     ('scaling', MinMaxScaler()),\n",
    "#     ('classify', KNeighborsClassifier())\n",
    "# ])\n",
    "# param_grid_DT = [\n",
    "#     {\n",
    "#         'reduce_dim': [SelectKBest()],\n",
    "#         'reduce_dim__k': [3, 6],\n",
    "#         'classify__algorithm' : ['auto'], \n",
    "#         'classify__leaf_size' : [30, 50],\n",
    "#         'classify__n_neighbors' : [5, 10], \n",
    "#         'classify__p' : [1, 2], \n",
    "#         'classify__weights' : ['uniform', 'distance'],\n",
    "#         'classify__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "# #gridsearch\n",
    "# grid = GridSearchCV(pipe, cv=sss, param_grid=param_grid_DT, scoring='recall')\n",
    "# gs = grid.fit(features_train, labels_train)\n",
    "# clf =gs.best_estimator_\n",
    "\n",
    "# # print clf    \n",
    "    \n",
    "# # #print the selected features:\n",
    "# # mask = (select.get_support())\n",
    "# # new_features = [] \n",
    "# # for bool, feature in zip(mask, features_list):\n",
    "# #     if bool:\n",
    "# #         new_features.append(feature)\n",
    "# # print new_features\n",
    "\n",
    "# #evaluation\n",
    "# # labels_predictions = grid.predict(scaled_feature_test)\n",
    "# # gbe = grid.best_estimator_\n",
    "# # print (\"Best parameters are: \", clf)\n",
    "# # pred = clf.predict(scaled_feature_test)\n",
    "# # acc= accuracy_score(pred, labels_test)\n",
    "# # print (\"Accuracy is:\",acc)\n",
    "# # print ( \"Recall is:\",  metrics.recall_score(labels_test, pred))\n",
    "# # print ( \"Precision is:\", metrics.precision_score(labels_test, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
