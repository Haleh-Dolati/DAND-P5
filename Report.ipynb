{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Identifying Fraud from Enron Emails and Financial Data\n",
    "### Haleh Dolati\n",
    "<br>\n",
    "## Project Overview\n",
    "Project Overview\n",
    "Enron Corporation was an American energy, commodities, and services company, which was formed in 1985 by, Kenneth Lay. This corporation filed for bankruptcy on December 2001. Enron case is famous for being the largest bankruptcy reorganization the biggest audit failure in American history at that time. <br>\n",
    "In this project, we are asked to use the Enron email data and identify employees who may have committed fraud and were involved in the Enron scam, using supervised machine learning. \n",
    "\n",
    "Machine learning algorithms are very useful to accomplish these kinds of goals, because their output is a machine system that builds models  without demanding human involvement which would be really time consuming.<br>\n",
    "The goal is to make a model that uses some financial and email data to identify Persons of Interest among the executives. The Person of Interest (POI) is defined as “_individuals who were indicted, reached a settlement or plea deal with the government, or testified in exchange for prosecution immunity_”. The mentioned information became public after the federal investigation of Enron scandal. Enron Email Dataset was prepared by the CALO Project in MIT and is available here: (https://www.cs.cmu.edu/~./enron/).  <br>\n",
    "\n",
    "This data has financial and email features as well as POI label which shoes whether an executive was POI as well. The features are: <br>\n",
    "**Financial features:** \n",
    "salary, deferral_payments, total_payments, loan_advances, bonus, restricted_stock_deferred, deferred_income, total_stock_value, expenses, exercised_stock_options, other', long_term_incentive, restricted_stock, director_fees <br>\n",
    "**Email features:** to_messages, email_address, from_poi_to_this_person, from_messages, from_this_person_to_poi, shared_receipt_with_poi <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Outliers\n",
    "The first step is to know the data and find and handle possible problems. First, I took a look at the name of the executives. Over all there are 146 executives in this data which 18 of them are POIs. The last two names **Total** and **The Travel Agency In The Park** are different from the rest. Total is the sum of all financial features therefore does not represent any executive just like The Travel Agency In The Park.  Therefore, I excluded them from the data. \n",
    "\n",
    "|Baxter John C|Chan Ronnie|Detmering Timothy J|Belfer Robert|\n",
    "| -------- |:-------------| :-----| \n",
    "|**Metts Mark**|**Kitchen Louise**|**Berberian David**|**Elliott Steven**|\n",
    "|**Cordes William R**|**Shankman Jeffrey A**|**Powers William**|**Wakeham John**|\n",
    "|**Hannon Kevin P**|**Wodraska John**|**Gold Joseph**|**Bannantine James M**|\n",
    "|**Mordaunt Kristina M**|**Bergsieker Richard P**|**Urquhart John A**|**Duncan John H**|\n",
    "|**Meyer Rockford G**|**Mcmahon Jeffrey**|**Bibi Philippe A**|**Shapiro Richard S**|\n",
    "|**Horton Stanley C**|**Rieker Paula H**|**Sherriff John R**|**Shelby Rex**|\n",
    "|**Piper Gregory F**|**Whaley David A**|**Beck Sally W**|**Lemaistre Charles**|\n",
    "|**Humphrey Gene E**|**Umanoff Adam S**|**Haug David L**|**Deffner Joseph M**|\n",
    "|**Blachman Jeremy M**|**Echols John B**|**Kishkill Joseph G**|**Whalley Lawrence G**|\n",
    "|**Sunde Martin**|**Mendelsohn John**|**Gibbs Dana R**|**Hickerson Gary J**|\n",
    "|**Lowry Charles P**|**Cline Kenneth W**|**Piro Jim**|**Mcconnell Michael S**|\n",
    "|**Colwell Wesley**|**Lewis Richard**|**Delainey David W**|**Sullivan-Shaklovitz Colleen**|\n",
    "|**Muller Mark S**|**Hayes Robert E**|**Mccarty Danny J**|**Wrobel Bruce**|\n",
    "|**Jackson Charlene R**|**Westfahl Richard K**|**Kopper Michael J**|**Lindholm Tod A**|\n",
    "|**Walters Gareth W**|**Leff Daniel P**|**Meyer Jerome J**|**Lay Kenneth L**|\n",
    "|**Walls Jr Robert H**|**Lavorato John J**|**Gathmann William D**|**Haedicke Mark E**|\n",
    "|**Butts Robert H**|**Olson Cindy K**|**Bowen Jr Raymond M**|**Gillis John**|\n",
    "|**Mcdonald Rebecca**|**Fitzgerald Jay L**|**Fitzgerald Jay L**|**Redmond Brian L**|\n",
    "|**Cumberland Michael S**|**Moran Michael P**|**Bazelides Philip J**|**Belden Timothy N**|\n",
    "|**Gahn Robert S**|**Mcclellan George**|**Duran William D**|**Thorn Terence H**|\n",
    "|**Hermann Robert J**|**Fastow Andrew S**|**Foy Joe**|**Fowler Peggy**|\n",
    "|**Scrimshaw Matthew**|**Kean Steven J**|**Wasaff George**|**White Jr Thomas E**|\n",
    "|**Calger Christopher F**|**Rice Kenneth D**|**Christodoulou Diomedes**|**Allen Phillip K**|\n",
    "|**Kaminski Wincenty J**|**Sharp Victoria T**|**Jaedicke Robert**|**Brown Michael**|\n",
    "|**Lockhart Eugene E**|**Winokur Jr. Herbert S**|**Badum James P**|**Hughes James A**|\n",
    "|**Cox David**|**Overdyke Jr Jere C**|**Reynolds Lawrence**|**Dimichele Richard G**|\n",
    "|**Pereira Paulo V. Ferraz**|**Bhatnagar Sanjay**|**Carter Rebecca C**|**Yeap Soon**|\n",
    "|**Stabler Frank**|**Buchanan Harold G**|**Murray Julia H**|**Garland C Kevin**|\n",
    "|**Skilling Jeffrey K**|**Blake Jr. Norman P**|**Dodson Keith**|**Yeager F Scott**|\n",
    "|**Sherrick Jeffrey B**|**Hirko Joseph**|**Dietrich Janet R**|**Prentice James**|\n",
    "|**Gray Rodney**|**Pai Lou L'**|**Bay Franklin R**|**Derrick Jr. James V**|\n",
    "|**Pickering Mark R**|**Hayslett Roderick J**|**Fugh John L**|**Frevert Mark A**|\n",
    "|**Noles James L**|**Fallon James B**|**Koenig Mark E**|**Martin Amanda K**|\n",
    "|**Izzo Lawrence L**|**Tilney Elizabeth A**|**Gramm Wendy L'**|**Causey Richard A**|\n",
    "|**Buy Richard B**|**Taylor Mitchell S**|**Donahue Jr Jeffrey M**|**Glisan Jr Ben F**|\n",
    "|**Savage Frank**|**<font color='red'>The Travel Agency In The Park</font>**|**<font color='red'>Total</font>**|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each executive is presented by 21 features. I went through them and using my intuition and some online research about the Enron scandal, I excluded some of them. At the end, I chose a mixture of financial and email data to be used in the feature selection part. This table shows all the available features and the one that I used. In the table below, the red ones are the excluded ones:\n",
    "\n",
    "|Feature's Name|Included? |Feature's Name|Included? |\n",
    "| -------- |:-------------:|-------- |:-------------:|\n",
    "|bonus|Yes|deferral_payments|Yes|\n",
    "|deferred_income|No|director_fees|No|\n",
    "|email_address|No|exercised_stock_options|Yes|\n",
    "|expenses|No|from_messages|No|\n",
    "|from_poi_to_this_person|Yes|from_this_person_to_poi|Yes|\n",
    "|loan_advances|No|long_term_incentive|No|\n",
    "|other|No|restricted_stock|No|\n",
    "|restricted_stock_deferred|Yes|salary|Yes|\n",
    "|shared_receipt_with_poi|No|total_payments|Yes|\n",
    "|total_stock_value|No|to_messages|No |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also looked into each features value to see whether there are missing values or anything out of ordinary. Of course there are features with very different value range, especially in financial features. However, I'm not going to exclude those extremely high values because many of them like extremely high bonuses actually can be an indicator of the fraud. \n",
    "Another step that I took was scaling the data. As mentioned before, there are two types of variables: financial and email. Email data has smaller numbers compared to financial data. Therefore, I've decided to scale them all.  \n",
    "\n",
    "\n",
    "|Features|Values |Features|Values |\n",
    "| -------- |:-------------|-------- |:-------------|\n",
    "|poi|144 non-null|bonus|81 non-null |\n",
    "|salary|94 non-null |salary_to_avg|94 non-null |\n",
    "|deferral_payments|38 non-null |total_payments|123 non-null |\n",
    "|restricted_stock_deferred|17 non-null |exercised_stock_options|101 non-null |\n",
    "|from_poi_to_this_person|86 non-null |from_this_person_to_poi|86 non-null |\n",
    "\n",
    "In addition to the mentioned features, I made a new feature by dividing the salary of each executive to the average of all salaries. This ratio shows how ones salary was way above the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the tuning step, I ran Gaussian Naive Bayes, K-Neighbors, and Decision Tree using their default parameters. While Gaussian Naive Bayes results were really underwhelming, the other two algorithms showed very good results.\n",
    "\n",
    "|Classifier|Accuracy |Recall|Precision |\n",
    "| -------- |:-------------|-------- |:-------------|\n",
    "|Gaussian Naive Bayes|0.28 |0.16| 1.0 |\n",
    "|K-Neighbors|0.82 |0.34| 0.25 |\n",
    "|Decision Tree|0.82 |0.4 |0.5 |\n",
    "\n",
    "As you can see, with the default parameters, decision tree achieved the best accuracy, recall and precision. However, K-Neighbors has the same accuracy and the other two metrics are not as bad as the Gaussian Naive Bayes. Based on these results, I decided to move forward with both K-Neighbors and Decision Tree and improve both of them with scaling and feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the parameters of a machine learning algorithm means to manually change and optimize the values of them in order to enable the lagorithm to perform the best. These parameters have a default value that will be applied if no other values has been assigned manually. For example, K-Neighbors has a parameter named weights, with the default value of 'uniform'. This parameter defines the weighitin system for the points. The default value of 'uniform' means all points in each neighborhood has the same weight. However, this parameter can take another pre-defined value: 'distance'. This value,'distance', assign the inverse of each point's distance as their weight.\n",
    "Here, I chose  K-Neighbors and Decision Tree for tuning. For Decision tree, I chose the following parameters to tune:\n",
    "\n",
    "### K-Neighbors\n",
    "|Parameter |Description |Values\n",
    "| -------- |:-------------|:-------- \n",
    "|leaf_size|number of cases or observations in that leaf. |30, 50\n",
    "|n_neighbors|Number of neighbors to use by default for k_neighbors queries | 5, 10\n",
    "|p|Power parameter for the Minkowski metric |1, 2 \n",
    "|weights|Weighiting system for the points |Distance, Uniform\n",
    "|algorithm|Algorithm used to compute the nearest neighbors |auto, ball_tree, kd_tree, brute\n",
    "\n",
    "### Decision Tree\n",
    "|Parameter |Description |Values\n",
    "| -------- |:-------------|:-------- \n",
    "|criterion|The function to measure the quality of a split |gini, balanced\n",
    "|class_weight|Weights associated with classes | None, Balanced\n",
    "|min_samples_split|The minimum number of samples required to split an internal node |2, 3, 4, 5 \n",
    "|max_leaf_nodes|Grow a tree with max_leaf_nodes in best-first fashion |None, 5\n",
    "|min_samples_leaf|The minimum number of samples required to be at a leaf node: |4, 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The followings are the codes for tuning of the two selected algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creat the pipeline\n",
    "sss = StratifiedShuffleSplit(n_splits=100, random_state = 42)\n",
    "pipe_KNC = Pipeline([\n",
    "    ('reduce_dim', SelectKBest()),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('classify', KNeighborsClassifier())\n",
    "])\n",
    "param_grid_KNC = [\n",
    "    {\n",
    "        'reduce_dim__k': [3, 6, 9],\n",
    "        'classify__algorithm' : ['auto'], \n",
    "        'classify__leaf_size' : [30, 50],\n",
    "        'classify__n_neighbors' : [5, 10], \n",
    "        'classify__p' : [1, 2], \n",
    "        'classify__weights' : ['uniform', 'distance'],\n",
    "        'classify__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    },\n",
    "]\n",
    "\n",
    "#gridsearch\n",
    "grid_KNC = GridSearchCV(pipe_KNC, cv=sss, param_grid=param_grid_KNC, scoring='recall')\n",
    "gs_KNC = grid_KNC.fit(features, labels)\n",
    "clf_KNC =gs_KNC.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creat the pipeline\n",
    "sss = StratifiedShuffleSplit(n_splits=100, random_state = 42)\n",
    "pipe = Pipeline([\n",
    "    ('reduce_dim', SelectKBest()),\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('classify', DecisionTreeClassifier(splitter='best', random_state=42))\n",
    "])\n",
    "\n",
    "param_grid_DT = [\n",
    "    {\n",
    "        'reduce_dim__k': [3,6,9],\n",
    "        'classify__criterion':['gini', 'entropy'],\n",
    "        'classify__class_weight':[None, 'balanced'],\n",
    "        'classify__min_samples_split' : [2,3,4,5],\n",
    "        'classify__max_leaf_nodes': [None, 5],\n",
    "        'classify__min_samples_leaf': [4, 2]\n",
    "    },\n",
    "]\n",
    "\n",
    "gs = GridSearchCV(pipe, cv=sss, param_grid=param_grid_DT, scoring='f1')\n",
    "gs.fit(features, labels)\n",
    "clf =gs.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SelectKBest chose 6 best feature to be included in the model. az it is shown in the following table, __bonus__ and __exercised_stock_options__ have significantly higher importance score compared to the rest:<br>\n",
    "\n",
    "- bonus: 0.35689672<br> \n",
    "- exercised_stock_options: 0.26079073 <br> \n",
    "- from_poi_to_this_person: 0.18264055<br>\n",
    "- salary_to_avg: 0.11818132<br> \n",
    "- total_payments: 0.08149068<br> \n",
    "- salary: 0. <br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The after tuning results shows althought K neighbors has slightly higher accuracy after tuning, Decision Tree has much better Precision, Recall, F1, and F2. Therefore, I chose it as the final tuned algorithm. \n",
    "\n",
    "\n",
    "|Classifier|Accuracy |Precision |Recall  |F1 |F2 |\n",
    "| -------- |:-------------|-------- |:-------------|--------|\n",
    "|KNeighborsClassifier|0.87 |0.54| 0.18 |0.27|0.21|\n",
    "|DecisionTreeClassifier|0.82 |0.35| 0.43 |0.38|0.41|\n",
    "\n",
    "Accuracy, recall and precision are the metric that i focused on for choosingmy algorithm. these three metrics are using values from confusion matrix to evaluate a n algorithm. Accuracy shows the how many of the predictted labes were predicted correctylr. In other words, accuracy is:\n",
    "\n",
    "$\\qquad$accuracy = (true positives + true negatives) / total predictions\n",
    "\n",
    "However, depending on the scenario that we want to predict, the false positive or false negative predictions might be far more important for us. For example, when it comes to medical tests, specially in the first steps, the preference is to predict one might has a type of cancer and rule it out by furthur tests (false positive) rather than missing someone who has cancer and finding out about it when it is too late (false negative). For these cases, we can take advantage of two other metrics: precision and recall.\n",
    "\n",
    "$\\qquad$precision = true positives / (true positives + false positives)\n",
    "\n",
    "$\\qquad$recall = true positives / (true positives + false negatives)\n",
    "\n",
    "The results of Decision Tree modes shows the accuracy of the model is 82%. it mean out of 100 predictions, 82 of them will be predicted correctly. The precision is 35% which means 35% of the predictions that were identified as POIs, were actually POIs. Finally, the 43% value for Recall the model catches 43% of the POIs.\n",
    "\n",
    "The SelectKBest chose 6 best feature to be included in the model. As it is shown in the following table, bonus and exercised_stock_options have significantly higher importance score compared to the rest:\n",
    "bonus: 0.35689672\n",
    "exercised_stock_options: 0.26079073 \n",
    "from_poi_to_this_person: 0.18264055\n",
    "salary_to_avg: 0.11818132\n",
    "total_payments: 0.08149068\n",
    "salary: 0."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
