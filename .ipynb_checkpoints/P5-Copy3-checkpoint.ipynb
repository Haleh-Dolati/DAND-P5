{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Haleh/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "import sys\n",
    "import pickle\n",
    "from tester import dump_classifier_and_data\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import RandomizedPCA\n",
    "from time import time\n",
    "import pylab as pl\n",
    "from feature_format import featureFormat\n",
    "from feature_format import targetFeatureSplit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load poi_id.py\n",
    "#!/usr/bin/python\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "\n",
    "features_list = ['poi',\n",
    "                 'salary',\n",
    "                 'deferral_payments',\n",
    "                 'total_payments',\n",
    "                 'loan_advances',\n",
    "                 'bonus',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'bonus_salary_R',\n",
    "                 'restricted_stock_deferred',\n",
    "                 'deferred_income',\n",
    "                 'total_stock_value',\n",
    "                 'expenses',\n",
    "                 'exercised_stock_options',\n",
    "                 'long_term_incentive',\n",
    "                 'restricted_stock',\n",
    "                 'director_fees',\n",
    "                 'to_messages',\n",
    "                 'from_poi_to_this_person',\n",
    "                 'from_poi_to_this_person_Percent',                 \n",
    "                 'from_messages',\n",
    "                 'from_this_person_to_poi',\n",
    "                 'from_this_person_to_poi_percent',\n",
    "                 'shared_receipt_with_poi']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Remove outliers\n",
    "based on mini-project in outlier lessons, i know there is a key \"Total\", which holds the total value for all other values. <br>\n",
    "Another one is the key \"THE TRAVEL AGENCY IN THE PARK\".<br>\n",
    "I'm going to exclude both of them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.pop(\"TOTAL\")\n",
    "data_dict.pop(\"THE TRAVEL AGENCY IN THE PARK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key, value in data_dict.iteritems():\n",
    "    if value['bonus'] == \"NaN\" or value['salary'] == \"Nan\":\n",
    "        value['bonus_salary_R'] = \"NaN\"\n",
    "    else:\n",
    "        value['bonus_salary_R'] = float(value['bonus']) / float(value['salary'])\n",
    "\n",
    "for key, value in data_dict.iteritems():\n",
    "    if value[\"from_this_person_to_poi\"] == \"NaN\" or value[\"to_messages\"] == \"Nan\":\n",
    "        value['from_this_person_to_poi_percent'] = \"NaN\"\n",
    "    else:\n",
    "        value['from_this_person_to_poi_percent'] = float(value['from_this_person_to_poi'])*100 / float(value['to_messages'])\n",
    "\n",
    "for key, value in data_dict.iteritems():\n",
    "    if value[\"from_poi_to_this_person\"] == \"NaN\" or value[\"to_messages\"] == \"Nan\":\n",
    "        value['from_poi_to_this_person_Percent'] = \"NaN\"\n",
    "    else:\n",
    "        value['from_poi_to_this_person_Percent'] = float(value['from_poi_to_this_person'])*100 / float(value['to_messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_dataset = data_dict\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 144 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 23 columns):\n",
      "poi                                144 non-null bool\n",
      "salary                             94 non-null float64\n",
      "deferral_payments                  38 non-null float64\n",
      "total_payments                     123 non-null float64\n",
      "loan_advances                      3 non-null float64\n",
      "bonus                              81 non-null float64\n",
      "restricted_stock_deferred          17 non-null float64\n",
      "bonus_salary_R                     81 non-null float64\n",
      "restricted_stock_deferred          17 non-null float64\n",
      "deferred_income                    48 non-null float64\n",
      "total_stock_value                  125 non-null float64\n",
      "expenses                           94 non-null float64\n",
      "exercised_stock_options            101 non-null float64\n",
      "long_term_incentive                65 non-null float64\n",
      "restricted_stock                   109 non-null float64\n",
      "director_fees                      16 non-null float64\n",
      "to_messages                        86 non-null float64\n",
      "from_poi_to_this_person            86 non-null float64\n",
      "from_poi_to_this_person_Percent    86 non-null float64\n",
      "from_messages                      86 non-null float64\n",
      "from_this_person_to_poi            86 non-null float64\n",
      "from_this_person_to_poi_percent    86 non-null float64\n",
      "shared_receipt_with_poi            86 non-null float64\n",
      "dtypes: bool(1), float64(22)\n",
      "memory usage: 26.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(my_dataset, orient = 'index')\n",
    "df = df[features_list]\n",
    "df = df.replace('NaN', np.nan)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Provided to give you a starting point. Try a variety of classifiers.\n",
    "data = featureFormat(data_dict, features_list)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels,  test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#minmax scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feature = scaler.fit_transform(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#PCA\n",
    "pca = PCA()\n",
    "pca.fit(features_train)\n",
    "features_train = pca.transform(features_train)\n",
    "features_test = pca.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are:  Pipeline(steps=[('reduce_dim', SelectKBest(k=5, score_func=<function chi2 at 0x10fc76848>)), ('classify', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=42, splitter='random'))])\n",
      "'The ' 5  features selected and their importances:\n",
      "feature no. 1: exercised_stock_options (1.0) (0.148314606742)\n",
      "feature no. 2: director_fees (0.0) (0.149097605777)\n",
      "feature no. 3: expenses (0.0) (0.00429259908966)\n",
      "feature no. 4: total_stock_value (0.0) (0.249556358986)\n",
      "feature no. 5: salary (0.0) (0.58009421758)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.61      0.72        36\n",
      "        1.0       0.22      0.57      0.32         7\n",
      "\n",
      "avg / total       0.77      0.60      0.66        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "pipe_DT = Pipeline([\n",
    "    ('reduce_dim', SelectKBest()),\n",
    "    ('classify', DecisionTreeClassifier(splitter='random', random_state=42))\n",
    "])\n",
    "\n",
    "N_FEATURES_OPTIONS = [5,10,15]\n",
    "C_OPTIONS = [1, 10, 100, 1000]\n",
    "max_features_options= ['auto', 'log2']\n",
    "criterion=['gini', 'entropy']\n",
    "max_depth=[None, 1, 2, 3, 4]\n",
    "class_weight=[None, 'balanced']\n",
    "param_grid_DT = [\n",
    "    {\n",
    "        'reduce_dim': [SelectKBest(chi2)],\n",
    "        'classify__criterion':['gini', 'entropy'],\n",
    "        'classify__max_depth':[None, 1, 2, 3, 4],\n",
    "        'classify__class_weight':[None, 'balanced'],\n",
    "        'reduce_dim__k': N_FEATURES_OPTIONS,\n",
    "        'classify__max_features': max_features_options},\n",
    "]\n",
    "reducer_labels = ['PCA', 'KBest(chi2)']\n",
    "\n",
    "grid = GridSearchCV(pipe_DT, cv=5, n_jobs=2, param_grid=param_grid_DT)\n",
    "grid.fit(scaled_feature, labels_train)\n",
    "\n",
    "labels_predictions = grid.predict(features_test)\n",
    "clf = grid.best_estimator_\n",
    "print (\"Best parameters are: \", clf)\n",
    "\n",
    "# Print features selected and their importances\n",
    "features_selected=[features_list[i+1] for i in clf.named_steps['reduce_dim'].get_support(indices=True)]\n",
    "scores = clf.named_steps['reduce_dim'].scores_\n",
    "importances = clf.named_steps['classify'].feature_importances_\n",
    "import numpy as np\n",
    "indices = np.argsort(importances)[::-1]\n",
    "print (\"'The '\", len(features_selected), \" features selected and their importances:\")\n",
    "for i in range(len(features_selected)):\n",
    "    print (\"feature no. {}: {} ({}) ({})\".format(i+1,features_selected[indices[i]],importances[indices[i]], scores[indices[i]]))\n",
    "\n",
    "# Print classification report (focus on precision and recall)\n",
    "report = classification_report( labels_test, labels_predictions )\n",
    "print(report)\n",
    "\n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
